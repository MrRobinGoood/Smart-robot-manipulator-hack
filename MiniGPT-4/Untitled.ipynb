{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5889cc41-6107-49b0-b558-55f2f3c334df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T12:32:08.264447Z",
     "iopub.status.busy": "2023-10-14T12:32:08.263368Z",
     "iopub.status.idle": "2023-10-14T12:32:13.820141Z",
     "shell.execute_reply": "2023-10-14T12:32:13.818842Z",
     "shell.execute_reply.started": "2023-10-14T12:32:08.264411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/MiniGPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'MiniGPT-4'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Vision-CAIR/MiniGPT-4.git\n",
    "%cd MiniGPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48ebef-7b90-473e-9f10-664642237b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T03:44:52.693633Z",
     "iopub.status.busy": "2023-10-15T03:44:52.692624Z",
     "iopub.status.idle": "2023-10-15T04:06:32.278820Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Git hooks.\n",
      "Git LFS initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama-2-7b-chat-hf'...\n",
      "Filtering content: 100% (3/3), 4.55 GiB | 3.59 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/daryl149/llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168a4fb4-8aed-4bdd-8ab7-1d16d10f6d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T07:24:09.981949Z",
     "iopub.status.busy": "2023-10-15T07:24:09.980938Z",
     "iopub.status.idle": "2023-10-15T07:24:10.029455Z",
     "shell.execute_reply": "2023-10-15T07:24:10.028219Z",
     "shell.execute_reply.started": "2023-10-15T07:24:09.981911Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/MiniGPT-4\n"
     ]
    }
   ],
   "source": [
    "%cd MiniGPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d5a27f-05c4-472e-8eb8-29ce1a6e4390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:17:41.606583Z",
     "iopub.status.busy": "2023-10-15T04:17:41.605369Z",
     "iopub.status.idle": "2023-10-15T04:17:50.411804Z",
     "shell.execute_reply": "2023-10-15T04:17:50.410413Z",
     "shell.execute_reply.started": "2023-10-15T04:17:41.606528Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate==0.20.3\n",
      "  Obtaining dependency information for accelerate==0.20.3 from https://files.pythonhosted.org/packages/10/d3/5382aa337d3e67214003a17b06bfc07cf0334356b4e8aaf3b12b0d38c83f/accelerate-0.20.3-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.10/site-packages (from accelerate==0.20.3) (23.2)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.10/site-packages (from accelerate==0.20.3) (5.7.3)\n",
      "Requirement already satisfied: pyyaml in /home/jupyter/.local/lib/python3.10/site-packages (from accelerate==0.20.3) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /kernel/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1)\n",
      "Requirement already satisfied: jinja2 in /kernel/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.20.3) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.20.3) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\n",
      "Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.18.0\n",
      "    Uninstalling accelerate-0.18.0:\n",
      "      Successfully uninstalled accelerate-0.18.0\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config and accelerate-launch are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed accelerate-0.20.3\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate==0.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f340ab4-84a4-44ae-9c1b-397241b35e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T07:24:32.362828Z",
     "iopub.status.busy": "2023-10-15T07:24:32.361858Z",
     "iopub.status.idle": "2023-10-15T07:24:35.283718Z",
     "shell.execute_reply": "2023-10-15T07:24:35.282536Z",
     "shell.execute_reply.started": "2023-10-15T07:24:32.362773Z"
    }
   },
   "outputs": [],
   "source": [
    "%rm -r llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b12347-328e-4f7c-b855-7405a3b4d91c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T05:36:03.654552Z",
     "iopub.status.busy": "2023-10-15T05:36:03.653466Z",
     "iopub.status.idle": "2023-10-15T05:41:07.285145Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 05:36:12.202088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/jupyter/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n",
      "Initializing Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/jupyter/.local/lib/python3.10/site-packages/cv2/../../lib64'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46a14d0d3b44ad29e3538d6d88d73bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.89G/1.89G [01:08<00:00, 29.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not use Q-Former here.\n",
      "Load MiniGPT-4 Checkpoint: /home/jupyter/datasphere/project/MiniGPT-4/pretrained_minigpt4_llama2_7b.pth\n",
      "Initialization Finished\n",
      "Running on public URL: https://24d3e5d77837234f9c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://24d3e5d77837234f9c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n",
      "debug device:  cuda:0\n",
      "debug model device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "%run -i demo.py --cfg-path eval_configs/minigpt4_llama2_eval.yaml  --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137b0d8-7238-449a-a095-d2ad413bfd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
